# -*- coding: utf-8 -*-
"""RecipeRecom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YnM0tUyWhhTQIXZipWdzIFfZGJHViFpi

**RECIPE RECOM**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import re
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

df = pd.read_csv('IndianFoodDatasetCSV.csv')

df.head()

df=df.drop(['RecipeName','Ingredients','Instructions'], axis=1)

df.columns

df=df.rename(columns={'TranslatedRecipeName' : 'RecipeName' , 'TranslatedIngredients' : 'Ingredients' , 'TranslatedInstructions' : 'Instructions'})

df.columns

df.shape

df.info()

df.describe()

df.dtypes

df.duplicated().sum()

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

df.dropna(inplace=True)

df.shape

df.duplicated().sum()

df[['CookTimeInMins','PrepTimeInMins','TotalTimeInMins']].isnull().sum()

df[['RecipeName','Ingredients']].duplicated().sum()

"""**VISULAISATION**"""

df['IngredientsNum']=df['Ingredients'].apply(lambda x: len(x.split(',')))

df.head()

df_tt=pd.cut(x=df['TotalTimeInMins'],bins=[0,15,30,40,60,120,240,1440,3000], labels=['0-15','15-30','30-44','45-60','60-120','120-240','240-1 day','1 day+'])

#Count total time in min
df_tt.value_counts().plot(kind='bar',color='purple')
plt.xlabel('Total Time in Mins')
plt.ylabel('Count')
plt.title('Total Time in Mins')
plt.show()

# Calculate the 95th percentile for each column
prep_time_95th = df['PrepTimeInMins'].quantile(0.95)
cook_time_95th = df['CookTimeInMins'].quantile(0.95)

# Filter out values above the 95th percentile
filtered_df = df[(df['PrepTimeInMins'] <= prep_time_95th) & (df['CookTimeInMins'] <= cook_time_95th)]

# Prep time in mins vs cook time iin mins
plt.figure(figsize=(10, 6))
x=plt.boxplot([filtered_df['PrepTimeInMins'], filtered_df['CookTimeInMins']], patch_artist=True, vert=True)
colors = ['green', 'lightgreen']

# Change patch colors
for patch, color in zip(x['boxes'], colors):
    patch.set_facecolor(color)
plt.xticks([1, 2], ['Prep Time In Mins', 'Cook Time In Mins'])
plt.ylabel('Time in Mins')
plt.title('Zoomed-in Boxplot (Excluding Extreme Outliers)')
plt.show()

#different courses
course_count=df['Course'].value_counts()
threshold=0.05*course_count.sum()
course_counts_mod=course_count[course_count>=threshold]
course_counts_mod['Others'] = course_count[course_count < threshold].sum()
course_counts_mod.plot(kind='pie', autopct='%1.1f%%', startangle=90,explode=[0.04,0.04,0.0,0.0,0.0,0.0,0.0])

#ingredients vs prep time
lower_bound=df['PrepTimeInMins'].quantile(0.01)
upper_bound=df['PrepTimeInMins'].quantile(0.99)
df['PrepTimeInMins'] = np.clip(df['PrepTimeInMins'], lower_bound, upper_bound)
fig, ax1 = plt.subplots(figsize=(12, 8))
ax1.scatter(df['PrepTimeInMins'],df['IngredientsNum'],color='g',marker='o')

df.head(3)

#different types of diet
dietPlot=df['Diet']
value_counts = dietPlot.value_counts()
threshold = 0.05 * value_counts.sum()
dietPlan_mod = value_counts[value_counts >= threshold]
dietPlan_mod['others']=value_counts[value_counts<threshold].sum()
dietPlan_mod.plot(kind='pie',autopct='%1.1f%%',startangle=90)

#common ingredients
text_ingd=' '.join(df['Ingredients'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_ingd)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Standardized Ingredients')
plt.show()

from collections import Counter
text_freq=' '.join(df['Ingredients'])
words=text_freq.split()
word_freq=Counter(words)
common_words=word_freq.most_common(10)
common_words

#frequeny of most common ingredients
df_common_words=pd.DataFrame(common_words,columns=['Words','Frequency'])
df_common_words.plot(kind='barh',x='Words',y='Frequency',color='seagreen',figsize=(10,6))
plt.xlabel('Common words')
plt.ylabel('Frequency')

"""**ML model**"""

df.isnull().sum()

df['TotalTimeInMins'].mean()

df['Ingredients'].head()

df['Ingredients'] = df['Ingredients'].apply(lambda x: re.sub(r'\d+', '', x))
df['Ingredients'].head()

df['Ingredients'] = df['Ingredients'].astype(str)
df['Ingredients']=df['Ingredients'].apply(lambda x: re.sub(r'[^\w\s]',' ',x).lower())
df['Ingredients'].head()

df['Ingredients']=df['Ingredients'].apply(word_tokenize)
df['Ingredients'].head()

stop_words=set(stopwords.words('english'))
df['Ingredients']=df['Ingredients'].apply(lambda x: [word for word in x if word not in stop_words])
df['Ingredients'].head()

lemmatizer = WordNetLemmatizer()
df['Ingredients'] = df['Ingredients'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

type(common_words)

common_words = common_words + ['cup', 'teaspoon', 'tablespoon', 'salt', 'taste', 'thinly', 'gram', 'chop', 'fine', "ounce", "gram", "pound", "chopped", "fresh", "ground", "large", "sliced", "peeled","cut", "freshly", "finely", "plus", "white", "clove", "room", "dry" , "inch","oil","per","use","split","cook","cooked","soak","minute","hour"]

common_words

common_words_set = set(common_words)
df['Ingredients']=df['Ingredients'].apply(lambda x: ' '.join([word for word in x if word not in common_words_set ]))
df['Ingredients'].head(10)

!pip install scikit-learn

tfv=TfidfVectorizer(min_df=5, max_features=1000, strip_accents='unicode', analyzer='word', token_pattern=r'\w{1,}', ngram_range=(1,2), use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')
df['Ingredients']=df['Ingredients'].fillna('')

df['Ingredients'] = df['Ingredients'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)

tfv_matrix_diet = tfv.fit_transform(df['Ingredients'])

def give_recipe_Rec(Ingredients):
  input_vector = tfv.transform([Ingredients])
  similarity_scores = cosine_similarity(input_vector, tfv_matrix_diet)
  top_indices = similarity_scores.argsort()[0][-5:]
  give_recipe_Rec = [(df.iloc[i]['RecipeName'], similarity_scores[0][i]) for i in reversed(top_indices)]
  for RecipeName, score in give_recipe_Rec:
      print(f"Recipe: {RecipeName}, Similarity Score: {score}")

give_recipe_Rec('egg spinach garlic')

